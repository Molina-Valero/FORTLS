---
title: "Tree-level"
#author: "Authors"
#date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tree-level}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

FORTLS is used for processing of TLS data. TLS data must be provided as .las or .laz files. The first obligatory step is the normalization of the point cloud applying the function `normalize`. The obtained normalized point clouds serve as input data for the tree detection functions i.e. `tree.detection.single.scan`, `tree.detection.multi.scan` and `tree.detection.several.plots`. The function `tree.detection.single.scan` detects trees from normalized TLS single-scan data and `tree.detection.multi.scan` from normalized TLS multi-scan data. If data from more than one plot is to be analyzed automatically, the function `tree.detection.several.plots` should be used. It includes both the normalization and the tree detection functions and executes these functions on each input plot sequentially. 

## Normalization

The aim of the normalization process is to calculate the coordinates of each point relative to the center of the point cloud. In this process, the functions `readLAS`, `clip_circle`, `classify_ground`, `grid_terrain` and `normalize_height` from the `lidR` package are used. The following steps are executed:

- Classification of points as "ground" or "not ground" by the cloth simulation filter (CSF) algorithm
- Generation of a digital terrain model (DTM) by spatial interpolation of the points classified as "ground". Two methods can be applied for the spatial interpolation, either the Delaunay triangulation (`tin`, by default) or a k-nearest neighbour approach with inverse-distance weighting (`knnidw`)
- Normalization of the point cloud by subtracting the DTM and computation of the Cartesian, cylindrical and spherical coordinates 
- Reduction of the density of the point cloud by the point cropping (PCP) algorithm

The `normalize` function is applied as follows:

```{r eval=FALSE, include=TRUE}
pcd <- normalize(las = "example_data.las", 
                 x.center = NULL, y.center = NULL,
                 max.dist = NULL, min.height = NULL, max.height = NULL, 
                 algorithm.dtm = "tin", res.dtm = 0.2,
                 csf = list(cloth_resolution = 0.5),
                 multiple.scans = NULL,
                 id = NULL, file = NULL,
                 dir.data = NULL, save.result = TRUE, dir.result = NULL)
```

The name of the .las or .laz file containing the TLS data is introduced in `las = ` and must include the .las/.laz extension.

The planimetric coordinates $x$ and $y$ of the center are by default `x.center = 0` and `y.center = 0`. If this does not coincide with the point cloud data, the coordinates of the plot center can be specified by `x.center` and `y.center`. 

Furthermore the size of the point cloud can optionally be reduced by the arguments `max.dist`, `min.height` and `max.height`. If the maximal horizontal distance in meter to the plot center(`max.dist`) is set, points that are further away are discarded. Similarly, the minimal and maximal height in meters (`min.height`, `max.height` respectively) can optionally define the point cloud. Those points that are lower than the minimal height or higher than the maximal height relative to the ground level are discarded. The default value for all three arguments is `NULL`. Hence, no points are discarded from the point cloud after normalization.

In order to generate the DTM, two different algorithms can be applied, which is specified by `algorithm.dtm = `. 

## Tree.detection

### Tree.detection.single.scan

### Tree.detection.multi.scan

### Tree.detection.several.plots

